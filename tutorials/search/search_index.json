{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"/*hide the duplicate tutorials heading*/ .md-nav__item--nested .md-nav__item--active .md-nav__link:first-of-type { display: none; } /*drop icon*/ .md-nav__link .md-nav__icon.md-icon { display: none; } .md-sidebar--secondary:not([hidden]) { visibility: hidden; } /*remove padding on tutorials posts*/ .md-nav__item--nested .md-nav__item--nested .md-nav .md-nav__list .md-nav__item { padding: 0; } Tutorials Deploying an image without a registry Deploying an image does not always require a remote registry, as shown in this short tutorial. Continue reading Creating a Datastore plugin Ever wanted to write a datastore plugin? This tutorial shows how we create official datastore plugins. Continue reading Automating Dokku Application Setup with Ansible This tutorial goes through the process of provisioning an app via ansible. Continue reading Run on an External Volume In order to leverage cloud-provider facilities like attachable volumes , ( a.k.a. block storage ) the following is an easy tutorial to achieve Dokku runs on them. Continue reading Deploying Gogs to Dokku Hot off the release of Dokku 0.6.3, here is a sweet tutorial made possible by the new port handling feature of Dokku. Continue reading","title":"Tutorials"},{"location":"#tutorials","text":"","title":"Tutorials"},{"location":"#deploying-an-image-without-a-registry","text":"Deploying an image does not always require a remote registry, as shown in this short tutorial. Continue reading","title":"Deploying an image without a registry"},{"location":"#creating-a-datastore-plugin","text":"Ever wanted to write a datastore plugin? This tutorial shows how we create official datastore plugins. Continue reading","title":"Creating a Datastore plugin"},{"location":"#automating-dokku-application-setup-with-ansible","text":"This tutorial goes through the process of provisioning an app via ansible. Continue reading","title":"Automating Dokku Application Setup with Ansible"},{"location":"#run-on-an-external-volume","text":"In order to leverage cloud-provider facilities like attachable volumes , ( a.k.a. block storage ) the following is an easy tutorial to achieve Dokku runs on them. Continue reading","title":"Run on an External Volume"},{"location":"#deploying-gogs-to-dokku","text":"Hot off the release of Dokku 0.6.3, here is a sweet tutorial made possible by the new port handling feature of Dokku. Continue reading","title":"Deploying Gogs to Dokku"},{"location":"apps/deploying-gogs-to-dokku/","tags":["dokku","ports","gogs","tutorial"],"text":"Hot off the release of Dokku 0.6.3, here is a sweet tutorial made possible by the new port handling feature of Dokku. If you're using Dokku - especially for commercial purposes - consider donating to project development via Github Sponsors , OpenCollective , or Patreon . Funds go to general development, support, and infrastructure costs. What is Gogs? Gogs (Go Git Service) is a painless self-hosted Git service. But what does that mean? Gogs is a self-hosted git management tool that can be used to host git repositories, issues, and releases on your own server. For this tutorial, we will be deploying gogs to a single Dokku server running at least Dokku 0.6.3. You may upgrade your Dokku instance to this version or install it from scratch. Please setup any ssh keys necessary for pushing applications, as we will not go over initial installation. Application Preparation Let's start by creating an application! While Dokku automatically creates an application on push, we will need to initialize some settings before the application will work, so it's best to do this manually. dokku apps:create gogs Before we continue, lets ensure that the proper domains and ports are setup for gogs. By default, the include Dockerfile exposes ports 3000 and 22 for the http and ssh processes, respectively. We will want our application to listen externally on port 80 , and will need to expose the ssh port on a different port as 22 is used by the host. We will not be using TCP load-balancing in our case, and instead will rely on the docker-options plugin to expose ssh properly. # expose container `http` port 3000 on host `http` port 80 dokku proxy:ports-add gogs http:80:3000 # expose the container port 22 on host port 2222 dokku docker-options:add gogs deploy -p 2222 :22 Next, we need to ensure there is persistent storage for Gogs. The Gogs docker image uses a directory mounted in /data , and we'll need either a docker volume or host directory to contain this data. For our purposes, we'll use a directory on the host. The official recommendation is to place persistent storage in the /var/lib/dokku/data/storage directory for cases where a user is not using a docker volume. As such, we'll create a subdirectory there for our application. # create the directory mkdir -p /var/lib/dokku/data/storage/gogs # ensure the proper permissions are set for the gogs directory chown -R dokku:dokku /var/lib/dokku/data/storage/gogs We can now mount the directory as persistent storage using the official storage plugin. The storage plugin does not check that the directory or volume exists, hence the need to create it beforehand. dokku storage:mount gogs /var/lib/dokku/data/storage/gogs:/data At this point, we need to setup our database for gogs. We will use the official dokku-mysql plugin, though you are welcome to use the dokku-postgres plugin or any other MySQL/Postgres installation you choose. dokku plugin:install https://github.com/dokku/dokku-mysql.git mysql dokku mysql:create gogs dokku mysql:link gogs gogs Pushing our Code Lets clone gogs locally. I have a ~/src directory in which I place all the applications I am currently working on and deploying, though any such directory should be fine. git clone git@github.com:gogits/gogs.git ~/src/gogs When pushing an application, you need to set the dokku host. For the purposes of this tutorial, the hostname of our dokku server is dokku.me . Note that the application name - gogs in this case - should be appended to the remote so that dokku knows what application you are pushing. git remote add dokku dokku@dokku.me:gogs And finally you can trigger a push of the gogs repository to your dokku server. This push will take a while as a few things need to happen: The actual repository needs to be pushed to your server The docker image must be built Not to worry though! Everything from this point on is cake :) git push dokku master Configuring Gogs Here is where it gets slightly tricky. You will want to use the following settings to configure Gogs: MySQL connection information can be retrieved from dokku mysql:info gogs Set the SSH port as 2222 . Gogs will use this to format your projects' SSH connection info in the UI. Do not change the application port. The application url should be changed to match your attached domain. In our case, it would be http://gogs.dokku.me/ The domain field should also be changed to match your attached domain, but without the 'http'. In our case, it would be gogs.dokku.me . This will also be used to format your projects' connection info the UI. Any of the optional settings can be configured as you wish. Once you submit the form, you should have a working Gogs Installation! Wrap-up As we displayed above, Dokku's rich featureset allows developers to quickly and easily setup applications as complex as a git management tool. With it's ability to deploy Dockerfile applications, proxy ports on the fly, and mount persistent storage, Dokku is a great tool to have in your deployment arsenal. Here's hoping it only gets better! If you're using Dokku - especially for commercial purposes - consider donating to project development via Github Sponsors , OpenCollective , or Patreon . Funds go to general development, support, and infrastructure costs.","title":"Deploying Gogs to Dokku"},{"location":"apps/deploying-gogs-to-dokku/#what-is-gogs","text":"Gogs (Go Git Service) is a painless self-hosted Git service. But what does that mean? Gogs is a self-hosted git management tool that can be used to host git repositories, issues, and releases on your own server. For this tutorial, we will be deploying gogs to a single Dokku server running at least Dokku 0.6.3. You may upgrade your Dokku instance to this version or install it from scratch. Please setup any ssh keys necessary for pushing applications, as we will not go over initial installation.","title":"What is Gogs?"},{"location":"apps/deploying-gogs-to-dokku/#application-preparation","text":"Let's start by creating an application! While Dokku automatically creates an application on push, we will need to initialize some settings before the application will work, so it's best to do this manually. dokku apps:create gogs Before we continue, lets ensure that the proper domains and ports are setup for gogs. By default, the include Dockerfile exposes ports 3000 and 22 for the http and ssh processes, respectively. We will want our application to listen externally on port 80 , and will need to expose the ssh port on a different port as 22 is used by the host. We will not be using TCP load-balancing in our case, and instead will rely on the docker-options plugin to expose ssh properly. # expose container `http` port 3000 on host `http` port 80 dokku proxy:ports-add gogs http:80:3000 # expose the container port 22 on host port 2222 dokku docker-options:add gogs deploy -p 2222 :22 Next, we need to ensure there is persistent storage for Gogs. The Gogs docker image uses a directory mounted in /data , and we'll need either a docker volume or host directory to contain this data. For our purposes, we'll use a directory on the host. The official recommendation is to place persistent storage in the /var/lib/dokku/data/storage directory for cases where a user is not using a docker volume. As such, we'll create a subdirectory there for our application. # create the directory mkdir -p /var/lib/dokku/data/storage/gogs # ensure the proper permissions are set for the gogs directory chown -R dokku:dokku /var/lib/dokku/data/storage/gogs We can now mount the directory as persistent storage using the official storage plugin. The storage plugin does not check that the directory or volume exists, hence the need to create it beforehand. dokku storage:mount gogs /var/lib/dokku/data/storage/gogs:/data At this point, we need to setup our database for gogs. We will use the official dokku-mysql plugin, though you are welcome to use the dokku-postgres plugin or any other MySQL/Postgres installation you choose. dokku plugin:install https://github.com/dokku/dokku-mysql.git mysql dokku mysql:create gogs dokku mysql:link gogs gogs","title":"Application Preparation"},{"location":"apps/deploying-gogs-to-dokku/#pushing-our-code","text":"Lets clone gogs locally. I have a ~/src directory in which I place all the applications I am currently working on and deploying, though any such directory should be fine. git clone git@github.com:gogits/gogs.git ~/src/gogs When pushing an application, you need to set the dokku host. For the purposes of this tutorial, the hostname of our dokku server is dokku.me . Note that the application name - gogs in this case - should be appended to the remote so that dokku knows what application you are pushing. git remote add dokku dokku@dokku.me:gogs And finally you can trigger a push of the gogs repository to your dokku server. This push will take a while as a few things need to happen: The actual repository needs to be pushed to your server The docker image must be built Not to worry though! Everything from this point on is cake :) git push dokku master","title":"Pushing our Code"},{"location":"apps/deploying-gogs-to-dokku/#configuring-gogs","text":"Here is where it gets slightly tricky. You will want to use the following settings to configure Gogs: MySQL connection information can be retrieved from dokku mysql:info gogs Set the SSH port as 2222 . Gogs will use this to format your projects' SSH connection info in the UI. Do not change the application port. The application url should be changed to match your attached domain. In our case, it would be http://gogs.dokku.me/ The domain field should also be changed to match your attached domain, but without the 'http'. In our case, it would be gogs.dokku.me . This will also be used to format your projects' connection info the UI. Any of the optional settings can be configured as you wish. Once you submit the form, you should have a working Gogs Installation!","title":"Configuring Gogs"},{"location":"apps/deploying-gogs-to-dokku/#wrap-up","text":"As we displayed above, Dokku's rich featureset allows developers to quickly and easily setup applications as complex as a git management tool. With it's ability to deploy Dockerfile applications, proxy ports on the fly, and mount persistent storage, Dokku is a great tool to have in your deployment arsenal. Here's hoping it only gets better! If you're using Dokku - especially for commercial purposes - consider donating to project development via Github Sponsors , OpenCollective , or Patreon . Funds go to general development, support, and infrastructure costs.","title":"Wrap-up"},{"location":"automation/automating-dokku-setup/","tags":["dokku","provisioning","automation"],"text":"When you deploy an app with Dokku, a common workflow is to create an app on git push : git remote add dokku dokku@dokku.me:app git push dokku master This works relatively well, and most folks then stumble through an initial app deploy/configuration cycle. In some cases, a user will create a script to encompass their workflow, or update some document to contain all the commands that were found necessary. However, this fails in at least the following two cases: The existing server fails in some way, and a new server must be provisioned to quickly service all requests. You need to replicate your deployment process on multiple servers/for multiple services. We'll evaluate two patterns to solve these problems, both of which are enabled by Dokku's porcelain interfaces. If you're using Dokku - especially for commercial purposes - consider donating to project development via Github Sponsors , OpenCollective , or Patreon . Funds go to general development, support, and infrastructure costs. Running Code on Server Boot Regardless of whether this is for a single replacement server, or if it is for a series of servers, running code to provision Dokku and necessary applications at boot time is ideal. Doing so will allow us to reduce the amount of time it takes to recover from service failure, as well as make it easier to do this on a fleet of servers. To this end, we can utilize User Data . \"User Data\" is a bit of configuration that can be run by a process called cloud-init . You should consider cloud-init to be the defacto server initialization tool for cloud servers; many popular server providers support it, such as Amazon Web Services, Azure, Digital Ocean, Google Cloud Provider, Linode, etc. Most folks provide user-data in bash script format, but there are many different modules to integrate with cloud-init . As an example, our own docs for Dreamhost support provide installation instructions in yaml format. Here is the simplest user-data for installing Dokku: #!/bin/bash wget https://raw.githubusercontent.com/dokku/dokku/master/bootstrap.sh sudo bash bootstrap.sh Cloud providers generally have a way to specify user data for either a single server or a set of servers being launched, though the method is different depending on the provider. If your provider does not support user data, our recommendation is to switch to one that does. ~~Creating~~ Provisioning an app automatically Taking this further, lets automatically create an app and configure it for deployment when a server starts #!/bin/bash wget https://raw.githubusercontent.com/dokku/dokku/master/bootstrap.sh sudo bash bootstrap.sh export APP_NAME = \"node-js-app\" dokku apps:exists \" $APP_NAME \" || dokku apps:create \" $APP_NAME \" dokku config:set \" $APP_NAME \" KEY = value Neat! One thing missing is the initial git clone, which would put our app into service. We can do that with the clone plugin: dokku plugin:install https://github.com/crisward/dokku-clone.git clone dokku clone \" $APP_NAME \" git@github.com:heroku/node-js-sample.git We'll be offering something even fancier soon, but props to Cris Ward for maintaining such a useful plugin! You now have a fully provisioned app on a new server on server boot. Your application downtime with this methodology decreases signficantly, and in many cases, this is enough to keep your business running. For folks using Dokku plugins for datastores, restoring service when all your data was stored on a non-existent server is a longer conversation with no easy solutions. At this time, none of the datastore plugins directly support running in HA mode, though this is something worth investigating. At this time, using managed datastore providers such as AWS RDS, CloudAMQP, etc. are the suggested methods for having HA datastore solutions. A brief introduction to Configuration Management Some of our users may be provisioning quite a few apps to a server, or the same server many times, or even managing a dozen servers for various clients. How do you handle that without a ton of bespoke bash scripts? How do you provision new applications without a tangle of if statements, in a DRY way? There are a few answers, but one common answer is to use a configuration management tool Configuration management tools provide common libraries and patterns for organizing server automation code. There are quite a few different tools in the config management space, but the one we're going show off is Ansible. Ansible requires python to run on a server. Assuming we're on an Ubuntu-based server, the following are roughly the installation instructions: sudo apt-add-repository -y ppa:ansible/ansible sudo apt-get update sudo apt-get install -y ansible Ansible provides an abstraction for executing python modules by writing small bits of yaml. Here is an example for running ansible against the local server. Place the following in a file called dokku.yml --- - hosts : dokku tasks : - name : dokku repo apt_repository : filename : dokku repo : 'deb https://packagecloud.io/dokku/dokku/ubuntu/ {{ ansible_lsb.codename|lower }} main' state : present - name : install dokku apt : pkg : dokku state : installed update_cache : true The above invokes python modules that takes the above as configuration and: creates an apt repository file for dokku ensures dokku is installed, updating the apt cache if apt isn't aware of it To run the above, we'll need to create a hosts file. I've created a dokku group with the IP of the server I'm going to target. [dokku] 127.0.0.1 Now that everything is setup, we can just run the following to execute our provisioning code: ansible-playbook -i hosts -s dokku.yml Provisioning many Dokku apps/servers with Ansible Now that we have a bare minimum ansible setup, we can iterate on this to provision actual Dokku applications. The following will create an app if it does not exist --- - hosts : dokku tasks : - name : does the node-js-app app exist shell : dokku apps:exists node-js-app register : app_exists ignore_errors : True - name : create an app shell : dokku apps:create node-js-app when : app_exists.rc == 1 This is pretty good so far, and uses the built-in shell Ansible libraries to do heavy lifting. However, the following would be much better: --- - hosts : dokku tasks : - name : dokku apps:create node-js-app dokku_app : app : node-js-app The above would use a custom dokku_app Ansible library for provisioning applications, building upon the porcelain we covered previously. While not in the scope of this blog post, I suggest anyone interested in doing so follow along the following tutorial . For our patreon followers, the code for the dokku_app library will be made available, as well as future plans around Ansible integration. Combining the methods into one Assuming we have a repository with our server provisioning code - the yaml and hosts files - we can use the following user-data for automatically setting up a dokku server on boot. #!/bin/bash # install ansible sudo apt-add-repository -y ppa:ansible/ansible sudo apt-get update sudo apt-get install -y ansible git # clone your infra repo git clone git@example.git:infra /tmp/infra # provision the server pushd /tmp/infra > /dev/null ansible-playbook -i hosts -s dokku.yml Going Further Once you have an infra repository containing the provisioning scripts for your servers, the next step is to do all Dokku configuration from this repository. This helps ensure migrating to a new server is as painless as possible, making service restoration a breeze. If you're using Dokku - especially for commercial purposes - consider donating to project development via Github Sponsors , OpenCollective , or Patreon . Funds go to general development, support, and infrastructure costs.","title":"Automating Dokku Application Setup with Ansible"},{"location":"automation/automating-dokku-setup/#running-code-on-server-boot","text":"Regardless of whether this is for a single replacement server, or if it is for a series of servers, running code to provision Dokku and necessary applications at boot time is ideal. Doing so will allow us to reduce the amount of time it takes to recover from service failure, as well as make it easier to do this on a fleet of servers. To this end, we can utilize User Data . \"User Data\" is a bit of configuration that can be run by a process called cloud-init . You should consider cloud-init to be the defacto server initialization tool for cloud servers; many popular server providers support it, such as Amazon Web Services, Azure, Digital Ocean, Google Cloud Provider, Linode, etc. Most folks provide user-data in bash script format, but there are many different modules to integrate with cloud-init . As an example, our own docs for Dreamhost support provide installation instructions in yaml format. Here is the simplest user-data for installing Dokku: #!/bin/bash wget https://raw.githubusercontent.com/dokku/dokku/master/bootstrap.sh sudo bash bootstrap.sh Cloud providers generally have a way to specify user data for either a single server or a set of servers being launched, though the method is different depending on the provider. If your provider does not support user data, our recommendation is to switch to one that does.","title":"Running Code on Server Boot"},{"location":"automation/automating-dokku-setup/#creating-provisioning-an-app-automatically","text":"Taking this further, lets automatically create an app and configure it for deployment when a server starts #!/bin/bash wget https://raw.githubusercontent.com/dokku/dokku/master/bootstrap.sh sudo bash bootstrap.sh export APP_NAME = \"node-js-app\" dokku apps:exists \" $APP_NAME \" || dokku apps:create \" $APP_NAME \" dokku config:set \" $APP_NAME \" KEY = value Neat! One thing missing is the initial git clone, which would put our app into service. We can do that with the clone plugin: dokku plugin:install https://github.com/crisward/dokku-clone.git clone dokku clone \" $APP_NAME \" git@github.com:heroku/node-js-sample.git We'll be offering something even fancier soon, but props to Cris Ward for maintaining such a useful plugin! You now have a fully provisioned app on a new server on server boot. Your application downtime with this methodology decreases signficantly, and in many cases, this is enough to keep your business running. For folks using Dokku plugins for datastores, restoring service when all your data was stored on a non-existent server is a longer conversation with no easy solutions. At this time, none of the datastore plugins directly support running in HA mode, though this is something worth investigating. At this time, using managed datastore providers such as AWS RDS, CloudAMQP, etc. are the suggested methods for having HA datastore solutions.","title":"~~Creating~~ Provisioning an app automatically"},{"location":"automation/automating-dokku-setup/#a-brief-introduction-to-configuration-management","text":"Some of our users may be provisioning quite a few apps to a server, or the same server many times, or even managing a dozen servers for various clients. How do you handle that without a ton of bespoke bash scripts? How do you provision new applications without a tangle of if statements, in a DRY way? There are a few answers, but one common answer is to use a configuration management tool Configuration management tools provide common libraries and patterns for organizing server automation code. There are quite a few different tools in the config management space, but the one we're going show off is Ansible. Ansible requires python to run on a server. Assuming we're on an Ubuntu-based server, the following are roughly the installation instructions: sudo apt-add-repository -y ppa:ansible/ansible sudo apt-get update sudo apt-get install -y ansible Ansible provides an abstraction for executing python modules by writing small bits of yaml. Here is an example for running ansible against the local server. Place the following in a file called dokku.yml --- - hosts : dokku tasks : - name : dokku repo apt_repository : filename : dokku repo : 'deb https://packagecloud.io/dokku/dokku/ubuntu/ {{ ansible_lsb.codename|lower }} main' state : present - name : install dokku apt : pkg : dokku state : installed update_cache : true The above invokes python modules that takes the above as configuration and: creates an apt repository file for dokku ensures dokku is installed, updating the apt cache if apt isn't aware of it To run the above, we'll need to create a hosts file. I've created a dokku group with the IP of the server I'm going to target. [dokku] 127.0.0.1 Now that everything is setup, we can just run the following to execute our provisioning code: ansible-playbook -i hosts -s dokku.yml","title":"A brief introduction to Configuration Management"},{"location":"automation/automating-dokku-setup/#provisioning-many-dokku-appsservers-with-ansible","text":"Now that we have a bare minimum ansible setup, we can iterate on this to provision actual Dokku applications. The following will create an app if it does not exist --- - hosts : dokku tasks : - name : does the node-js-app app exist shell : dokku apps:exists node-js-app register : app_exists ignore_errors : True - name : create an app shell : dokku apps:create node-js-app when : app_exists.rc == 1 This is pretty good so far, and uses the built-in shell Ansible libraries to do heavy lifting. However, the following would be much better: --- - hosts : dokku tasks : - name : dokku apps:create node-js-app dokku_app : app : node-js-app The above would use a custom dokku_app Ansible library for provisioning applications, building upon the porcelain we covered previously. While not in the scope of this blog post, I suggest anyone interested in doing so follow along the following tutorial . For our patreon followers, the code for the dokku_app library will be made available, as well as future plans around Ansible integration.","title":"Provisioning many Dokku apps/servers with Ansible"},{"location":"automation/automating-dokku-setup/#combining-the-methods-into-one","text":"Assuming we have a repository with our server provisioning code - the yaml and hosts files - we can use the following user-data for automatically setting up a dokku server on boot. #!/bin/bash # install ansible sudo apt-add-repository -y ppa:ansible/ansible sudo apt-get update sudo apt-get install -y ansible git # clone your infra repo git clone git@example.git:infra /tmp/infra # provision the server pushd /tmp/infra > /dev/null ansible-playbook -i hosts -s dokku.yml","title":"Combining the methods into one"},{"location":"automation/automating-dokku-setup/#going-further","text":"Once you have an infra repository containing the provisioning scripts for your servers, the next step is to do all Dokku configuration from this repository. This helps ensure migrating to a new server is as painless as possible, making service restoration a breeze. If you're using Dokku - especially for commercial purposes - consider donating to project development via Github Sponsors , OpenCollective , or Patreon . Funds go to general development, support, and infrastructure costs.","title":"Going Further"},{"location":"other/deploying-an-image-without-a-registry/","tags":["dokku","deploying"],"text":"To deploy an image from CI without an intermediate registry, we can run the following series of commands. Note This tutorial assumes the app being deployed is named node-js-app and the Dokku server's hostname is dokku.me . Please modify these values as appropriate. Building the image First, we'll assume the image is built. The following is one example for building a docker image, though your setup may vary. The image repository must not be dokku/ , as that namespace is used internally for tagging images by Dokku. # a good tag to use is the commit sha of your build docker image build --tag app/node-js-app:2935cc3d . Loading the image onto the host Next, we save the image to a file. This can be done with the docker image save command: docker image save --ouput node-js-app.tar The image must then be loaded on the remote server. This should be performed with the docker load command, and must be performed by a user that has access to the docker daemon. Note that because this command is not exposed by Dokku, a user other than dokku must be used for the ssh command. cat node-js-app.tar | ssh root@dokku.me \"docker load\" Alternatively, you can save the image and load it in one command like so: docker image save | ssh root@dokku.me \"docker load\" Deploying the image Finally, we can deploy the image using the git:from-image Dokku command. ssh dokku@dokku.me git:from-image node-js-app app/node-js-app:2935cc3d","title":"Deploying an image without a registry"},{"location":"other/deploying-an-image-without-a-registry/#building-the-image","text":"First, we'll assume the image is built. The following is one example for building a docker image, though your setup may vary. The image repository must not be dokku/ , as that namespace is used internally for tagging images by Dokku. # a good tag to use is the commit sha of your build docker image build --tag app/node-js-app:2935cc3d .","title":"Building the image"},{"location":"other/deploying-an-image-without-a-registry/#loading-the-image-onto-the-host","text":"Next, we save the image to a file. This can be done with the docker image save command: docker image save --ouput node-js-app.tar The image must then be loaded on the remote server. This should be performed with the docker load command, and must be performed by a user that has access to the docker daemon. Note that because this command is not exposed by Dokku, a user other than dokku must be used for the ssh command. cat node-js-app.tar | ssh root@dokku.me \"docker load\" Alternatively, you can save the image and load it in one command like so: docker image save | ssh root@dokku.me \"docker load\"","title":"Loading the image onto the host"},{"location":"other/deploying-an-image-without-a-registry/#deploying-the-image","text":"Finally, we can deploy the image using the git:from-image Dokku command. ssh dokku@dokku.me git:from-image node-js-app app/node-js-app:2935cc3d","title":"Deploying the image"},{"location":"other/run-on-external-volume/","tags":["dokku","provisioning"],"text":"In order to leverage cloud-provider facilities like attachable volumes , ( a.k.a. block storage ) the following is an easy tutorial to achieve Dokku runs on them. Warning: If the block storage is not available and attached on boot it is possible that containers will not correctly start. Please keep this in mind when considering moving Dokku and/or Docker to network attached storage. The following is intended to be executed on the dokku host machine as root . Say, for instance , that our volume is mapped into the systems as /dev/vdb1 . Stop docker daemon systemctl stop docker Prepare the filesystem: mkfs -t ext4 /dev/vdb1 mkdir /mnt/volume mount /dev/vdb1 /mnt/volume Move the old data directories: mv /home/dokku /home/dokku.OLD mv /var/lib/docker /var/lib/docker.OLD mv /var/lib/dokku /var/lib/dokku.OLD Move the data on the volume mkdir /mnt/volume/home/ mkdir -p /mnt/volume/var/lib/ mv /home/dokku.OLD /mnt/volume/home/dokku mv /var/lib/dokku.OLD /mnt/volume/var/lib/dokku mv /var/lib/docker.OLD /mnt/volume/var/lib/docker Prepare the mountpoints mkdir /home/dokku mkdir /var/lib/dokku mkdir /var/lib/docker chown dokku:dokku /home/dokku # respect the original ownership chmod 711 /var/lib/docker # respect the original permissions Mount bind mount -o bind /mnt/volume/home/dokku /home/dokku mount -o bind /mnt/volume/var/lib/dokku /var/lib/dokku mount -o bind /mnt/volume/var/lib/docker /var/lib/docker Start docker daemon systemctl start docker At this point all should be working fine, please check it out. Then, let the changes be reboot-persistent echo '/dev/vdb1 /mnt/volume ext4 defaults 0 2' | sudo tee -a /etc/fstab echo '/mnt/volume/home/dokku /home/dokku none defaults,bind 0 0' | sudo tee -a /etc/fstab echo '/mnt/volume/var/lib/dokku /var/lib/dokku none defaults,bind 0 0' | sudo tee -a /etc/fstab echo '/mnt/volume/var/lib/docker /var/lib/docker none defaults,bind 0 0' | sudo tee -a /etc/fstab","title":"Run on an External Volume"},{"location":"plugins/creating-a-datastore-plugin/","tags":["dokku","plugins"],"text":"Ever wanted to write a datastore plugin? This tutorial shows how we create official datastore plugins. If you're using Dokku - especially for commercial purposes - consider donating to project development via Github Sponsors , OpenCollective , or Patreon . Funds go to general development, support, and infrastructure costs. Initializing the plugin First, we'll start by cloning an existing datastore plugin. I'll choose the postgres plugin. git clone https://github.com/dokku/dokku-postgres dokku-meillisearch cd dokku-meillisearch rm -rf .git Once this is done, we can modify the plugin.toml to have the following contents: [plugin] description = \"dokku meillisearch service plugin\" version = \"0.0.1\" [plugin.config] Reconfiguring the plugin Now begins the great find/replace. We'll do the following case-sensitive replacements: postgres meillisearch Postgres Meillisearch POSTGRES MEILLISEARCH After the find/replace, we'll need to configure the plugin correctly. There should be a config file that contains runtime configuration for the plugin. We'll need to change the following in this file: PLUGIN_DATASTORE_PORTS : A bash array containing a list of ports exposed by the service. In many cases, this will be a single port. All ports should be listed here. The first port in the list is used when linking the service to an app. For meillisearch , the config should look as follows export PLUGIN_DATASTORE_PORTS=(7700) PLUGIN_DATASTORE_WAIT_PORT : This should be the primary port that Dokku waits to be ready when creating the service. Usually this will be the port used to link the service to an app. For meillisearch , the config should look as follows export PLUGIN_DATASTORE_WAIT_PORT=7700 PLUGIN_SCHEME : The value of this is used in the DSN. Do not leave it blank. It can usually be the lowercase version of the datastore name, but sometimes may need to be something different. For meillisearch , the config should look as follows export PLUGIN_SCHEME=http PLUGIN_DEFAULT_ALIAS : This is used as the prefix for the _URL value. DATABASE_URL is pretty normal for sql datastores. For meillisearch , the config should look as follows` export PLUGIN_DEFAULT_ALIAS=\"MEILLISEARCH\" The config file also contains a PLUGIN_UNIMPLEMENTED_SUBCOMMANDS shell array. This contains a list of commands that are not supported by the plugin. For sql-like plugins, this is usually empty, but sometimes certain functionality is not supported by the datastore (such as backup/restore or connecting to the service using a repl). In our case, we'll set the following value. export PLUGIN_UNIMPLEMENTED_SUBCOMMANDS =( \"backup\" \"backup-auth\" \"backup-deauth\" \"backup-schedule\" \"backup-schedule-cat\" \"backup-set-encryption\" \"backup-unschedule\" \"backup-unset-encryption\" \"clone\" \"connect\" \"export\" \"import\" ) This can always be revisited in the future as functionality becomes available to the datastore. Once this is set, we need to update the default Docker image the datastore plugin will use. This is contained within the Dockerfile. As of the time of writing, the latest stable image tag is v0.23.1 , so we'll have the following as the Dockerfile contents: FROM getmeili/meilisearch:v0.23.1 These cover the general changes. Now on to function updates. Customizing Commands 90% of the plugin is templated, but datastore-specific functions are stored in the functions file. We'll go over each of these below (and describe the customizations for meilisearch ). service_connect : Connects to the datastore via a repl. The repl must be available in the base image in use, and not any customization. For meillisearch , we'll replace the existing psql call with dokku_log_fail \"Not yet implemented\" service_create : Usually only customized if there are password needs (either the datastore doesn't support a password or supports a root password in addition to the normal one). For meillisearch , we don't need to customize anything. service_create_container : The meat and potatos. This creates the container and intiailizes data for the container (if necessary). For meillisearch , we can drop the code that initiliazes container database (from ~line 96-105, which contains the service_port_unpause call). Additionally, the ID=$(docker run ...) command should become the following: ID=$(docker run --name \"$SERVICE_NAME\" $MEMORY_LIMIT $SHM_SIZE -v \"$SERVICE_HOST_ROOT/data:/data.ms\" -e \"MEILI_MASTER_KEY=$PASSWORD\" -e \"MEILI_HTTP_ADDR=0.0.0.0:7700\" -e \"MEILI_NO_ANALYTICS=true\" --env-file=\"$SERVICE_ROOT/ENV\" -d --restart always --label dokku=service --label dokku.service=meillisearch \"$PLUGIN_IMAGE:$PLUGIN_IMAGE_VERSION\" $CONFIG_OPTIONS) service_export : Used for exporting the service data. You can implement this if the container has some way to export the data to stdout For meillisearch , we'll replace the existing psql call with dokku_log_fail \"Not yet implemented\" service_import : Analogous to service_export , used for importing the service data. You can implement this if the container has some way import the data from stdin. For meillisearch , we'll replace the existing psql call with dokku_log_fail \"Not yet implemented\" service_start : The only time this is customized is when the service either has no passwords (so the password check is removed) or has a secondary, root password (so we add another check). For meillisearch , the existing checks the Postgres plugin performs are enough. service_url : This outputs the default DSN-formatted connection string. Docker exposes other variables containing just IPs, PORTs, and other values from the config, so it is heavily encouraged to not come up with your own format here. For meillisearch , this should become the following: echo \"$PLUGIN_SCHEME://:$PASSWORD@$SERVICE_DNS_HOSTNAME:${PLUGIN_DATASTORE_PORTS[0]}\" Fixing tests Usually the following should be modified for tests. Below contains the changes for our meillisearch plugin. Tests matching unimplemented commands should be removed. For meillisearch , this means deleting the following files: tests/service_clone.bats tests/service_connect.bats tests/service_export.bats tests/service_import.bats Port references should be updated. In our case, a find/replace of 5432 with 7700 is enough for this. username:password need to conform to how the datastore works. For meillisearch , we can do two find-replacements: //u:p => //:p //meillisearch:$password => //:$password The plugin scheme should be updated. This is done with two find/replace calls: meillisearch:// => http:// meillisearch2 => http2 The \"database\" in the DSN should be updated to match the plugin's service_url format. In our case, we'll need a few find-replacements: /db\" => \" (basically removing the suffix) /l\" => \" (basically removing the suffix) /test_with_underscores\" => \" (basically removing the suffix) /db : there will be one instance of this in a config:set call. The string should just be removed. The dsn key should be updated to match the PLUGIN_DEFAULT_ALIAS . For meillisearch , we can do the following find/replace: DATABASE_URL => MEILLISEARCH_URL Regenerating the README.md The readme is generated by reading through the plugin source and generating help based on the config file and the source of each subcommand. It is enhanced by files in the docs folder. For our use case, we'll remove everything in the docs folder except for docs/README. This can be done in a single call to bin/generate a script included with each plugin that requries python3 . Commiting everything If everything went well, we can commit and push our new service plugin to Github. The plugin should automatically run tests in Github Actions, at which point you can catch any lingering errors. If you're using Dokku - especially for commercial purposes - consider donating to project development via Github Sponsors , OpenCollective , or Patreon . Funds go to general development, support, and infrastructure costs.","title":"Creating a Datastore plugin"},{"location":"plugins/creating-a-datastore-plugin/#initializing-the-plugin","text":"First, we'll start by cloning an existing datastore plugin. I'll choose the postgres plugin. git clone https://github.com/dokku/dokku-postgres dokku-meillisearch cd dokku-meillisearch rm -rf .git Once this is done, we can modify the plugin.toml to have the following contents: [plugin] description = \"dokku meillisearch service plugin\" version = \"0.0.1\" [plugin.config]","title":"Initializing the plugin"},{"location":"plugins/creating-a-datastore-plugin/#reconfiguring-the-plugin","text":"Now begins the great find/replace. We'll do the following case-sensitive replacements: postgres meillisearch Postgres Meillisearch POSTGRES MEILLISEARCH After the find/replace, we'll need to configure the plugin correctly. There should be a config file that contains runtime configuration for the plugin. We'll need to change the following in this file: PLUGIN_DATASTORE_PORTS : A bash array containing a list of ports exposed by the service. In many cases, this will be a single port. All ports should be listed here. The first port in the list is used when linking the service to an app. For meillisearch , the config should look as follows export PLUGIN_DATASTORE_PORTS=(7700) PLUGIN_DATASTORE_WAIT_PORT : This should be the primary port that Dokku waits to be ready when creating the service. Usually this will be the port used to link the service to an app. For meillisearch , the config should look as follows export PLUGIN_DATASTORE_WAIT_PORT=7700 PLUGIN_SCHEME : The value of this is used in the DSN. Do not leave it blank. It can usually be the lowercase version of the datastore name, but sometimes may need to be something different. For meillisearch , the config should look as follows export PLUGIN_SCHEME=http PLUGIN_DEFAULT_ALIAS : This is used as the prefix for the _URL value. DATABASE_URL is pretty normal for sql datastores. For meillisearch , the config should look as follows` export PLUGIN_DEFAULT_ALIAS=\"MEILLISEARCH\" The config file also contains a PLUGIN_UNIMPLEMENTED_SUBCOMMANDS shell array. This contains a list of commands that are not supported by the plugin. For sql-like plugins, this is usually empty, but sometimes certain functionality is not supported by the datastore (such as backup/restore or connecting to the service using a repl). In our case, we'll set the following value. export PLUGIN_UNIMPLEMENTED_SUBCOMMANDS =( \"backup\" \"backup-auth\" \"backup-deauth\" \"backup-schedule\" \"backup-schedule-cat\" \"backup-set-encryption\" \"backup-unschedule\" \"backup-unset-encryption\" \"clone\" \"connect\" \"export\" \"import\" ) This can always be revisited in the future as functionality becomes available to the datastore. Once this is set, we need to update the default Docker image the datastore plugin will use. This is contained within the Dockerfile. As of the time of writing, the latest stable image tag is v0.23.1 , so we'll have the following as the Dockerfile contents: FROM getmeili/meilisearch:v0.23.1 These cover the general changes. Now on to function updates.","title":"Reconfiguring the plugin"},{"location":"plugins/creating-a-datastore-plugin/#customizing-commands","text":"90% of the plugin is templated, but datastore-specific functions are stored in the functions file. We'll go over each of these below (and describe the customizations for meilisearch ). service_connect : Connects to the datastore via a repl. The repl must be available in the base image in use, and not any customization. For meillisearch , we'll replace the existing psql call with dokku_log_fail \"Not yet implemented\" service_create : Usually only customized if there are password needs (either the datastore doesn't support a password or supports a root password in addition to the normal one). For meillisearch , we don't need to customize anything. service_create_container : The meat and potatos. This creates the container and intiailizes data for the container (if necessary). For meillisearch , we can drop the code that initiliazes container database (from ~line 96-105, which contains the service_port_unpause call). Additionally, the ID=$(docker run ...) command should become the following: ID=$(docker run --name \"$SERVICE_NAME\" $MEMORY_LIMIT $SHM_SIZE -v \"$SERVICE_HOST_ROOT/data:/data.ms\" -e \"MEILI_MASTER_KEY=$PASSWORD\" -e \"MEILI_HTTP_ADDR=0.0.0.0:7700\" -e \"MEILI_NO_ANALYTICS=true\" --env-file=\"$SERVICE_ROOT/ENV\" -d --restart always --label dokku=service --label dokku.service=meillisearch \"$PLUGIN_IMAGE:$PLUGIN_IMAGE_VERSION\" $CONFIG_OPTIONS) service_export : Used for exporting the service data. You can implement this if the container has some way to export the data to stdout For meillisearch , we'll replace the existing psql call with dokku_log_fail \"Not yet implemented\" service_import : Analogous to service_export , used for importing the service data. You can implement this if the container has some way import the data from stdin. For meillisearch , we'll replace the existing psql call with dokku_log_fail \"Not yet implemented\" service_start : The only time this is customized is when the service either has no passwords (so the password check is removed) or has a secondary, root password (so we add another check). For meillisearch , the existing checks the Postgres plugin performs are enough. service_url : This outputs the default DSN-formatted connection string. Docker exposes other variables containing just IPs, PORTs, and other values from the config, so it is heavily encouraged to not come up with your own format here. For meillisearch , this should become the following: echo \"$PLUGIN_SCHEME://:$PASSWORD@$SERVICE_DNS_HOSTNAME:${PLUGIN_DATASTORE_PORTS[0]}\"","title":"Customizing Commands"},{"location":"plugins/creating-a-datastore-plugin/#fixing-tests","text":"Usually the following should be modified for tests. Below contains the changes for our meillisearch plugin. Tests matching unimplemented commands should be removed. For meillisearch , this means deleting the following files: tests/service_clone.bats tests/service_connect.bats tests/service_export.bats tests/service_import.bats Port references should be updated. In our case, a find/replace of 5432 with 7700 is enough for this. username:password need to conform to how the datastore works. For meillisearch , we can do two find-replacements: //u:p => //:p //meillisearch:$password => //:$password The plugin scheme should be updated. This is done with two find/replace calls: meillisearch:// => http:// meillisearch2 => http2 The \"database\" in the DSN should be updated to match the plugin's service_url format. In our case, we'll need a few find-replacements: /db\" => \" (basically removing the suffix) /l\" => \" (basically removing the suffix) /test_with_underscores\" => \" (basically removing the suffix) /db : there will be one instance of this in a config:set call. The string should just be removed. The dsn key should be updated to match the PLUGIN_DEFAULT_ALIAS . For meillisearch , we can do the following find/replace: DATABASE_URL => MEILLISEARCH_URL","title":"Fixing tests"},{"location":"plugins/creating-a-datastore-plugin/#regenerating-the-readmemd","text":"The readme is generated by reading through the plugin source and generating help based on the config file and the source of each subcommand. It is enhanced by files in the docs folder. For our use case, we'll remove everything in the docs folder except for docs/README. This can be done in a single call to bin/generate a script included with each plugin that requries python3 .","title":"Regenerating the README.md"},{"location":"plugins/creating-a-datastore-plugin/#commiting-everything","text":"","title":"Commiting everything"},{"location":"plugins/creating-a-datastore-plugin/#if-everything-went-well-we-can-commit-and-push-our-new-service-plugin-to-github-the-plugin-should-automatically-run-tests-in-github-actions-at-which-point-you-can-catch-any-lingering-errors","text":"If you're using Dokku - especially for commercial purposes - consider donating to project development via Github Sponsors , OpenCollective , or Patreon . Funds go to general development, support, and infrastructure costs.","title":"If everything went well, we can commit and push our new service plugin to Github. The plugin should automatically run tests in Github Actions, at which point you can catch any lingering errors."}]}